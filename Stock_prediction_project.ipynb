{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1) Implementing the Neural network from scratch**"
      ],
      "metadata": {
        "id": "2bJIr-JQxhnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqgAWzB5BPrr"
      },
      "outputs": [],
      "source": [
        "#Building a neural network\n",
        "#Training the model using the boston data set\n",
        "#Import the necessary libraries\n",
        "import numpy as np\n",
        "import random as random\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "#defining the activation key\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
        "\n",
        "\n",
        "#training the model with parameters\n",
        "def train(X, y, n_hidden, learning_rate, n_iter):\n",
        "    m, n_input = X.shape\n",
        "    #randomly assigned weights and biases\n",
        "    W1 = np.random.randn(n_input, n_hidden)\n",
        "    b1 = np.zeros((1, n_hidden))\n",
        "    W2 = np.random.randn(n_hidden, 1)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    for i in range(1, n_iter + 1):\n",
        "        Z2 = np.matmul(X, W1) + b1\n",
        "        A2 = sigmoid(Z2)\n",
        "        Z3 = np.matmul(A2, W2) + b2\n",
        "        A3 = Z3\n",
        "        dZ3 = A3 - y\n",
        "        dW2 = np.matmul(A2.T, dZ3)\n",
        "        db2 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "        dZ2 = np.matmul(dZ3, W2.T) * sigmoid_derivative(Z2)\n",
        "        dW1 = np.matmul(X.T, dZ2)\n",
        "        db1 = np.sum(dZ2, axis=0)\n",
        "        W2 = W2 - learning_rate * dW2 / m\n",
        "        b2 = b2 - learning_rate * db2 / m\n",
        "        W1 = W1 - learning_rate * dW1 / m\n",
        "        b1 = b1 - learning_rate * db1 / m\n",
        "        if i % 100 == 0:\n",
        "            cost = np.mean((y - A3) ** 2)\n",
        "            print('iteration %i, training loss %f' %\n",
        "                  (i, cost))\n",
        "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset is loaded\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "boston = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]"
      ],
      "metadata": {
        "id": "Cyu5RTswxJHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with the scaled dataset we are defining the X_test, X_train, y_train, y_test\n",
        "num_test = 10  # the last 10 samples as testing set\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train = boston[:, :-1][:-num_test, :]\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "y_train = target[:-num_test].reshape(-1, 1)\n",
        "X_test = boston[:, :-1][-num_test:, :]\n",
        "X_test = scaler.transform(X_test)\n",
        "y_test = target[-num_test:]"
      ],
      "metadata": {
        "id": "oEBzwTQIxNjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training one hidden layer with one dataset.\n",
        "n_hidden = 20\n",
        "learning_rate = 0.1\n",
        "n_iter = 2000\n",
        "model = train(X_train, y_train, n_hidden, learning_rate, n_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhqO4OZxTxy",
        "outputId": "58ec9c00-e998-4676-e876-0eaa3bcc9838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 100, training loss 18.071608\n",
            "iteration 200, training loss 14.217944\n",
            "iteration 300, training loss 12.580607\n",
            "iteration 400, training loss 11.479846\n",
            "iteration 500, training loss 10.627573\n",
            "iteration 600, training loss 9.939789\n",
            "iteration 700, training loss 9.378834\n",
            "iteration 800, training loss 8.903415\n",
            "iteration 900, training loss 8.480662\n",
            "iteration 1000, training loss 8.099411\n",
            "iteration 1100, training loss 7.752708\n",
            "iteration 1200, training loss 7.450111\n",
            "iteration 1300, training loss 7.136181\n",
            "iteration 1400, training loss 6.780490\n",
            "iteration 1500, training loss 6.579860\n",
            "iteration 1600, training loss 6.396434\n",
            "iteration 1700, training loss 6.226977\n",
            "iteration 1800, training loss 6.070564\n",
            "iteration 1900, training loss 5.914313\n",
            "iteration 2000, training loss 5.758366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we define prediction model which takes model input to produce the regression results\n",
        "def predict(x, model):\n",
        "    W1 = model['W1']\n",
        "    b1 = model['b1']\n",
        "    W2 = model['W2']\n",
        "    b2 = model['b2']\n",
        "    A2 = sigmoid(np.matmul(x, W1) + b1)\n",
        "    A3 = np.matmul(A2, W2) + b2\n",
        "    return A3"
      ],
      "metadata": {
        "id": "fuvFeHtzxXp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we apply trained model to test the model\n",
        "predictions = predict(X_test, model)\n",
        "print(predictions)\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KN3nFPZBiwC",
        "outputId": "252ad1ce-b970-489d-de97-c18c620532c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16.9511088 ]\n",
            " [19.32031228]\n",
            " [22.20059306]\n",
            " [18.91502203]\n",
            " [20.46553723]\n",
            " [29.80617691]\n",
            " [22.0284508 ]\n",
            " [31.6243103 ]\n",
            " [29.020395  ]\n",
            " [20.74838031]]\n",
            "[19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9 22.  11.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Implementing neural network with Scikit learn**\n"
      ],
      "metadata": {
        "id": "-eOa7gFJxphx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural networks with scikit learn\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Create an instance of the MLPRegressor class with the desired hyperparameters\n",
        "nn_scikit = MLPRegressor(hidden_layer_sizes=(16, 8), activation='relu', solver='adam', learning_rate_init=0.001, random_state=42, max_iter=2000)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nn_scikit.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "predictions = nn_scikit.predict(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIHAFsRCJijg",
        "outputId": "09836437-5a46-400e-ffe5-43d4a94e7bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17.35432868 18.38962029 21.50634442 19.22683883 19.72729684 22.71390889\n",
            " 20.49468318 22.97956332 21.71043007 19.94366541]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print the MSE of the prediction\n",
        "print(np.mean(y_test- predictions)**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvmPY1hqJ1ob",
        "outputId": "b5ea7ba0-f36a-4727-e33b-acb335d5b4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9499776967234388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Implemeting neural network with Tensorflow**"
      ],
      "metadata": {
        "id": "gPkwPJAs9lB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#random seed is added as results are reproducible.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#model is defined\n",
        "model = keras.Sequential([keras.layers.Dense(units= 20, activation = 'relu'),\n",
        "                          keras.layers.Dense(units= 8, activation = 'relu'),\n",
        "                          keras.layers.Dense(units= 1)])\n",
        "\n",
        "#Compiling the model\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.02))\n",
        "#train the model\n",
        "model.fit(X_train, y_train, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0yTupckLVip",
        "outputId": "fbd6adba-9adb-4a9f-ed4a-c82695f6b664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 475.2203\n",
            "Epoch 2/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 110.5561\n",
            "Epoch 3/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.1194\n",
            "Epoch 4/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 29.8767\n",
            "Epoch 5/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 24.6369\n",
            "Epoch 6/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 23.6489\n",
            "Epoch 7/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 20.5556\n",
            "Epoch 8/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 19.6276\n",
            "Epoch 9/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 17.8804\n",
            "Epoch 10/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 17.1301\n",
            "Epoch 11/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 16.9353\n",
            "Epoch 12/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 16.8396\n",
            "Epoch 13/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 16.7952\n",
            "Epoch 14/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 16.5400\n",
            "Epoch 15/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 15.1698\n",
            "Epoch 16/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.2101\n",
            "Epoch 17/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.6555\n",
            "Epoch 18/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.4548\n",
            "Epoch 19/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.8784\n",
            "Epoch 20/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.9620\n",
            "Epoch 21/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.4692\n",
            "Epoch 22/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.5692\n",
            "Epoch 23/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 14.1321\n",
            "Epoch 24/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.9758\n",
            "Epoch 25/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.7858\n",
            "Epoch 26/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.7072\n",
            "Epoch 27/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.5563\n",
            "Epoch 28/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.4177\n",
            "Epoch 29/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.3558\n",
            "Epoch 30/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.8430\n",
            "Epoch 31/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.5478\n",
            "Epoch 32/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.4863\n",
            "Epoch 33/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.2961\n",
            "Epoch 34/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.8530\n",
            "Epoch 35/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.6443\n",
            "Epoch 36/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.1860\n",
            "Epoch 37/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.9162\n",
            "Epoch 38/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.6318\n",
            "Epoch 39/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.0629\n",
            "Epoch 40/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.3926\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.7226\n",
            "Epoch 42/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.0761\n",
            "Epoch 43/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.4322\n",
            "Epoch 44/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 11.1965\n",
            "Epoch 45/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.8462\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.8307\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.6658\n",
            "Epoch 48/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.2218\n",
            "Epoch 49/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.1173\n",
            "Epoch 50/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.4110\n",
            "Epoch 51/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 11.4115\n",
            "Epoch 52/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.1616\n",
            "Epoch 53/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.3247\n",
            "Epoch 54/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.1913\n",
            "Epoch 55/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 10.6814\n",
            "Epoch 56/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.0089\n",
            "Epoch 57/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.5596\n",
            "Epoch 58/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.3626\n",
            "Epoch 59/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.9827\n",
            "Epoch 60/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.6451\n",
            "Epoch 61/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.9002\n",
            "Epoch 62/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.3252\n",
            "Epoch 63/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 10.2754\n",
            "Epoch 64/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.6000\n",
            "Epoch 65/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 9.7888\n",
            "Epoch 66/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 9.7293\n",
            "Epoch 67/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.5364\n",
            "Epoch 68/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.5106\n",
            "Epoch 69/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 10.7320\n",
            "Epoch 70/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 10.0738\n",
            "Epoch 71/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.1404\n",
            "Epoch 72/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.1680\n",
            "Epoch 73/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.5181\n",
            "Epoch 74/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 8.7901\n",
            "Epoch 75/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.8480\n",
            "Epoch 76/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.4986\n",
            "Epoch 77/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 8.5475\n",
            "Epoch 78/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.4239\n",
            "Epoch 79/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.3398\n",
            "Epoch 80/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 10.0837\n",
            "Epoch 81/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.7167\n",
            "Epoch 82/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.2272\n",
            "Epoch 83/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 11.3074\n",
            "Epoch 84/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.7360\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.4712\n",
            "Epoch 86/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.7271\n",
            "Epoch 87/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.6872\n",
            "Epoch 88/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.0145\n",
            "Epoch 89/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.7009\n",
            "Epoch 90/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.8593\n",
            "Epoch 91/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.1070\n",
            "Epoch 92/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.8892\n",
            "Epoch 93/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.3641\n",
            "Epoch 94/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.2591\n",
            "Epoch 95/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.9311\n",
            "Epoch 96/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.6895\n",
            "Epoch 97/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 10.9971\n",
            "Epoch 98/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 10.6799\n",
            "Epoch 99/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.9196\n",
            "Epoch 100/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.9590\n",
            "Epoch 101/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.6563\n",
            "Epoch 102/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.1901\n",
            "Epoch 103/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.6795\n",
            "Epoch 104/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.4705\n",
            "Epoch 105/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.2322\n",
            "Epoch 106/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.1792\n",
            "Epoch 107/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.1145\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.6219\n",
            "Epoch 109/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.1692\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 7.8908\n",
            "Epoch 111/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.3140\n",
            "Epoch 112/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.2239\n",
            "Epoch 113/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.2885\n",
            "Epoch 114/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.4422\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.7275\n",
            "Epoch 116/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.6591\n",
            "Epoch 117/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.2054\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.8406\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.5169\n",
            "Epoch 120/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 8.0648\n",
            "Epoch 121/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.7420\n",
            "Epoch 122/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.5579\n",
            "Epoch 123/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.9008\n",
            "Epoch 124/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 7.5238\n",
            "Epoch 125/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.4391\n",
            "Epoch 126/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7357\n",
            "Epoch 127/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.0909\n",
            "Epoch 128/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.3398\n",
            "Epoch 129/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6505\n",
            "Epoch 130/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.5532\n",
            "Epoch 131/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.0711\n",
            "Epoch 132/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.3716\n",
            "Epoch 133/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.2277\n",
            "Epoch 134/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0890\n",
            "Epoch 135/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.6606\n",
            "Epoch 136/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7578\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.2184\n",
            "Epoch 138/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7310\n",
            "Epoch 139/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5877\n",
            "Epoch 140/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.0797\n",
            "Epoch 141/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.8287\n",
            "Epoch 142/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1629\n",
            "Epoch 143/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6958\n",
            "Epoch 144/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.6262\n",
            "Epoch 145/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.4077\n",
            "Epoch 146/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0269\n",
            "Epoch 147/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4435\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5433\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.2834\n",
            "Epoch 150/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3155\n",
            "Epoch 151/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0261\n",
            "Epoch 152/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9199\n",
            "Epoch 153/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2748\n",
            "Epoch 154/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.0802\n",
            "Epoch 155/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.2662\n",
            "Epoch 156/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.7208\n",
            "Epoch 157/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.6702\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6662\n",
            "Epoch 159/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1725\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4133\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.5787\n",
            "Epoch 162/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6974\n",
            "Epoch 163/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2410\n",
            "Epoch 164/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0701\n",
            "Epoch 165/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0009\n",
            "Epoch 166/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.0094\n",
            "Epoch 167/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2051\n",
            "Epoch 168/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4123\n",
            "Epoch 169/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2582\n",
            "Epoch 170/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.5159\n",
            "Epoch 171/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9627\n",
            "Epoch 172/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9475\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9050\n",
            "Epoch 174/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0945\n",
            "Epoch 175/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7850\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9744\n",
            "Epoch 177/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3374\n",
            "Epoch 178/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.2464\n",
            "Epoch 179/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.3022\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4387\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8655\n",
            "Epoch 182/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2590\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7312\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7032\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.3395\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.6000\n",
            "Epoch 187/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3324\n",
            "Epoch 188/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8525\n",
            "Epoch 189/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2570\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0809\n",
            "Epoch 191/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4532\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5618\n",
            "Epoch 193/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4156\n",
            "Epoch 194/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2977\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0634\n",
            "Epoch 196/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4863\n",
            "Epoch 197/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8532\n",
            "Epoch 198/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5240\n",
            "Epoch 199/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1052\n",
            "Epoch 200/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3360\n",
            "Epoch 201/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5492\n",
            "Epoch 202/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7528\n",
            "Epoch 203/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1595\n",
            "Epoch 204/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8635\n",
            "Epoch 205/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9245\n",
            "Epoch 206/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1787\n",
            "Epoch 207/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6999\n",
            "Epoch 208/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4322\n",
            "Epoch 209/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2723\n",
            "Epoch 210/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8888\n",
            "Epoch 211/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4667\n",
            "Epoch 212/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.1582\n",
            "Epoch 213/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4764\n",
            "Epoch 214/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3010\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9525\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8875\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3956\n",
            "Epoch 218/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2150\n",
            "Epoch 219/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4146\n",
            "Epoch 220/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3837\n",
            "Epoch 221/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6369\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2869\n",
            "Epoch 223/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.6387\n",
            "Epoch 224/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7820\n",
            "Epoch 225/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3282\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7452\n",
            "Epoch 227/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0483\n",
            "Epoch 228/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8600\n",
            "Epoch 229/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.1541\n",
            "Epoch 230/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0520\n",
            "Epoch 231/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7947\n",
            "Epoch 232/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.9372\n",
            "Epoch 233/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6769\n",
            "Epoch 234/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8406\n",
            "Epoch 235/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7226\n",
            "Epoch 236/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3225\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0907\n",
            "Epoch 238/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3950\n",
            "Epoch 239/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4046\n",
            "Epoch 240/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4528\n",
            "Epoch 241/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7396\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4645\n",
            "Epoch 243/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.7763\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4198\n",
            "Epoch 245/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2702\n",
            "Epoch 246/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4171\n",
            "Epoch 247/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.0316\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0707\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.3609\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4237\n",
            "Epoch 251/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0219\n",
            "Epoch 252/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8708\n",
            "Epoch 253/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6997\n",
            "Epoch 254/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4017\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.6002\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4560\n",
            "Epoch 257/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4952\n",
            "Epoch 258/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2290\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.6534\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7305\n",
            "Epoch 261/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3948\n",
            "Epoch 262/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.5844\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2012\n",
            "Epoch 264/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3315\n",
            "Epoch 265/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2183\n",
            "Epoch 266/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.5842\n",
            "Epoch 267/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0626\n",
            "Epoch 268/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3441\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3195\n",
            "Epoch 270/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4023\n",
            "Epoch 271/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2241\n",
            "Epoch 272/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8850\n",
            "Epoch 273/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9239\n",
            "Epoch 274/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.0012\n",
            "Epoch 275/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.4543\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.6028\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2452\n",
            "Epoch 278/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.9215\n",
            "Epoch 279/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5951\n",
            "Epoch 280/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.9610\n",
            "Epoch 281/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.5411\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.1113\n",
            "Epoch 283/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.5115\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4261\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.5748\n",
            "Epoch 286/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.0797\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8953\n",
            "Epoch 288/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.1238\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.9521\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6373\n",
            "Epoch 291/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5298\n",
            "Epoch 292/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5738\n",
            "Epoch 293/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4226\n",
            "Epoch 294/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2760\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.6534\n",
            "Epoch 296/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.2485\n",
            "Epoch 297/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.7261\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.2055\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2156\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.2035\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f862d753370>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the predictions\n",
        "predictions = model.predict(X_test)[:, 0]\n",
        "print(predictions)\n",
        "\n",
        "#printing the loss function\n",
        "print(np.mean((y_test- predictions))**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn1tazN8Le_j",
        "outputId": "8f6ff4c7-ec6f-4271-af83-0b9627e1590b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 92ms/step\n",
            "[20.439594 21.477303 23.614979 22.104256 21.855715 24.898129 26.357567\n",
            " 21.142498 21.048439 25.026201]\n",
            "11.333106605167044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Preventing overfitting in the neural network**"
      ],
      "metadata": {
        "id": "MGwC_vnG0kJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preventing the overfitting\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(units=32, activation ='relu'), tf.keras.layers.Dropout(0.5), keras.layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "2CqriJIlLnNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting the stock prices of dow jones**"
      ],
      "metadata": {
        "id": "YDr0CYSUQaqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the stock prices with neural network\n",
        "import pandas as pd\n",
        "mydata = pd.read_csv('/content/20051201-20051210.csv', index_col = 'Date')\n",
        "mydata "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "dKpLuqIrUlaY",
        "outputId": "f8e8f7f1-5a30-4baf-9d20-3f5f7b10aa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Open     High      Low    Close     Volume\n",
              "Date                                                     \n",
              "2005-12-01  10806.0  10934.9  10806.0  10912.6  256932865\n",
              "2005-12-02  10912.0  10921.4  10861.7  10877.5  214888854\n",
              "2005-12-05  10877.0  10877.0  10810.7  10835.0  237430947\n",
              "2005-12-06  10835.4  10936.2  10835.4  10856.9  264721465\n",
              "2005-12-07  10856.9  10868.1  10764.0  10810.9  243543206\n",
              "2005-12-08  10808.4  10847.2  10729.7  10755.1  253313750\n",
              "2005-12-09  10751.8  10806.0  10729.9  10778.6  238907145"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b16d5fd0-b397-4b77-8a7f-07e23ffce8a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2005-12-01</th>\n",
              "      <td>10806.0</td>\n",
              "      <td>10934.9</td>\n",
              "      <td>10806.0</td>\n",
              "      <td>10912.6</td>\n",
              "      <td>256932865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-02</th>\n",
              "      <td>10912.0</td>\n",
              "      <td>10921.4</td>\n",
              "      <td>10861.7</td>\n",
              "      <td>10877.5</td>\n",
              "      <td>214888854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-05</th>\n",
              "      <td>10877.0</td>\n",
              "      <td>10877.0</td>\n",
              "      <td>10810.7</td>\n",
              "      <td>10835.0</td>\n",
              "      <td>237430947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-06</th>\n",
              "      <td>10835.4</td>\n",
              "      <td>10936.2</td>\n",
              "      <td>10835.4</td>\n",
              "      <td>10856.9</td>\n",
              "      <td>264721465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-07</th>\n",
              "      <td>10856.9</td>\n",
              "      <td>10868.1</td>\n",
              "      <td>10764.0</td>\n",
              "      <td>10810.9</td>\n",
              "      <td>243543206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-08</th>\n",
              "      <td>10808.4</td>\n",
              "      <td>10847.2</td>\n",
              "      <td>10729.7</td>\n",
              "      <td>10755.1</td>\n",
              "      <td>253313750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005-12-09</th>\n",
              "      <td>10751.8</td>\n",
              "      <td>10806.0</td>\n",
              "      <td>10729.9</td>\n",
              "      <td>10778.6</td>\n",
              "      <td>238907145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b16d5fd0-b397-4b77-8a7f-07e23ffce8a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b16d5fd0-b397-4b77-8a7f-07e23ffce8a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b16d5fd0-b397-4b77-8a7f-07e23ffce8a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing feature generation by starting with a sub-function that directly creates features from the original six features\n",
        "def add_original_feature(df, df_new):\n",
        "  df_new['open']= df['Open']\n",
        "  df_new['open_1']= df['Open'].shift(1)\n",
        "  df_new['close_1']= df['Close'].shift(1)\n",
        "  df_new['high_1']= df['High'].shift(1)\n",
        "  df_new['low_1']= df['Low'].shift(1)\n",
        "  df_new['volume_1']= df['Volume'].shift(1)\n",
        "#develop a sub-function that generates six features related to average close prices\n",
        "def add_avg_price(df, df_new):\n",
        "  df_new['avg_price_5']= df['Close'].rolling(5).mean().shift(1)\n",
        "  df_new['avg_price_30']= df['Close'].rolling(21).mean().shift(1)\n",
        "  df_new['avg_price_365']= df['Close'].rolling(252).mean().shift(1)\n",
        "  df_new['ratio_avg_price_5_30'] = df_new['avg_price_5']/df_new['avg_price_30']\n",
        "  df_new['ratio_avg_price_5_365'] = df_new['avg_price_5']/df_new['avg_price_365']\n",
        "  df_new['ratio_avg_price_30_365'] = df_new['avg_price_30']/df_new['avg_price_365']\n",
        "\n",
        "#a sub-function that generates six features related to average volumes is as follows\n",
        "def add_avg_volume(df, df_new):\n",
        "  df_new['avg_volume_5']= df['Volume'].rolling(5).mean().shift(1)\n",
        "  df_new['avg_volume_30']= df['Volume'].rolling(21).mean().shift(1)\n",
        "  df_new['avg_volume_365']= df['Volume'].rolling(252).mean().shift(1)\n",
        "  df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5']/df_new['avg_volume_30']\n",
        "  df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5']/df_new['avg_volume_365']\n",
        "  df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30']/df_new['avg_volume_365']\n",
        "\n",
        "#for the standard deviation, we develop the following sub-function for the price related features\n",
        "def add_std_price(df, df_new):\n",
        "  df_new['std_price_5']= df['Close'].rolling(5).std().shift(1)\n",
        "  df_new['std_price_30']= df['Close'].rolling(21).std().shift(1)\n",
        "  df_new['std_price_365']= df['Close'].rolling(252).std().shift(1)\n",
        "  df_new['ratio_std_price_5_30'] = df_new['std_price_5']/df_new['std_price_30']\n",
        "  df_new['ratio_std_price_5_365'] = df_new['std_price_5']/df_new['std_price_365']\n",
        "  df_new['ratio_std_price_30_365'] = df_new['std_price_30']/df_new['std_price_365']\n",
        "\n",
        "#a sub-function that generates six volume-based standard deviation features is as follows.\n",
        "\n",
        "def add_std_volume(df, df_new):\n",
        "  df_new['std_volume_5']= df['Volume'].rolling(5).std().shift(1)\n",
        "  df_new['std_volume_30']= df['Volume'].rolling(21).std().shift(1)\n",
        "  df_new['std_volume_365']= df['Volume'].rolling(252).std().shift(1)\n",
        "  df_new['ratio_std_volume_5_30'] = df_new['std_volume_5']/df_new['std_volume_30']\n",
        "  df_new['ratio_std_volume_5_365'] = df_new['std_volume_5']/df_new['std_volume_365']\n",
        "  df_new['ratio_std_volume_30_365'] = df_new['std_volume_30']/df_new['std_volume_365']\n",
        "\n",
        "#Seven return-based features are generated using the following sub-function.\n",
        "\n",
        "def add_return_feature(df, df_new):\n",
        "  df_new['return_1']= ((df['Close']- df['Close']).shift(1))/df['Close'].shift(1).shift(1)\n",
        "  df_new['return_5']= ((df['Close']-df['Close']).shift(5))/df['Close'].shift(5).shift(1)\n",
        "  df_new['return_30']= ((df['Close']-df['Close']).shift(21))/df['Close'].shift(21).shift(1)\n",
        "  df_new['return_365']= ((df['Close']-df['Close']).shift(252))/df['Close'].shift(252).shift(1)\n",
        "  df_new['moving_avg_5'] = df_new['return_1'].rolling(5).mean().shift(1)\n",
        "  df_new['moving_avg_30'] = df_new['return_1'].rolling(21).mean().shift(1)\n",
        "  df_new['moving_avg_365'] = df_new['return_1'].rolling(252).mean().shift(1)\n",
        "\n",
        "#main feature generation function\n",
        "\n",
        "def generate_features(df):\n",
        "  df_new = pd.DataFrame()\n",
        "  #6 original features\n",
        "  add_original_feature(df, df_new)\n",
        "  #31 genrated features\n",
        "  add_avg_price(df, df_new)\n",
        "  add_avg_volume(df, df_new)\n",
        "  add_std_price(df, df_new)\n",
        "  add_std_volume(df, df_new)\n",
        "  add_return_feature(df, df_new)\n",
        "  # the target\n",
        "  df_new['close'] = df ['Close']\n",
        "  df_new = df_new.dropna(axis=0)\n",
        "  return df_new"
      ],
      "metadata": {
        "id": "Ve6QEpWbOMgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "data_raw = pd.read_csv('/content/19880101-20191231.csv', index_col ='Date')\n",
        "data = generate_features(data_raw)\n",
        "print(data.round(decimals=3).head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjAtXJVOOstx",
        "outputId": "99bb8645-3246-4ef3-cf81-0592c4493de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              open  open_1  close_1  high_1   low_1    volume_1  avg_price_5  \\\n",
            "Date                                                                           \n",
            "1989-01-04  2146.6  2168.4   2144.6  2168.4  2127.1  17302883.0      2165.00   \n",
            "1989-01-05  2177.7  2146.6   2177.7  2183.4  2146.6  15714720.0      2168.00   \n",
            "1989-01-06  2190.5  2177.7   2190.5  2205.2  2173.0  20303094.0      2172.82   \n",
            "1989-01-09  2194.3  2190.5   2194.3  2213.8  2182.3  16494441.0      2175.14   \n",
            "1989-01-10  2199.5  2194.3   2199.5  2209.1  2185.0  18410324.0      2181.32   \n",
            "\n",
            "            avg_price_30  avg_price_365  ratio_avg_price_5_30  ...  \\\n",
            "Date                                                           ...   \n",
            "1989-01-04      2150.624       2062.113                 1.007  ...   \n",
            "1989-01-05      2154.690       2062.668                 1.006  ...   \n",
            "1989-01-06      2157.867       2063.218                 1.007  ...   \n",
            "1989-01-09      2160.005       2064.341                 1.007  ...   \n",
            "1989-01-10      2162.190       2065.351                 1.009  ...   \n",
            "\n",
            "            ratio_std_volume_5_365  ratio_std_volume_30_365  return_1  \\\n",
            "Date                                                                    \n",
            "1989-01-04                   0.563                    0.723       0.0   \n",
            "1989-01-05                   0.474                    0.724       0.0   \n",
            "1989-01-06                   0.580                    0.748       0.0   \n",
            "1989-01-09                   0.516                    0.746       0.0   \n",
            "1989-01-10                   0.279                    0.742       0.0   \n",
            "\n",
            "            return_5  return_30  return_365  moving_avg_5  moving_avg_30  \\\n",
            "Date                                                                       \n",
            "1989-01-04       0.0        0.0         0.0           0.0            0.0   \n",
            "1989-01-05       0.0        0.0         0.0           0.0            0.0   \n",
            "1989-01-06       0.0        0.0         0.0           0.0            0.0   \n",
            "1989-01-09       0.0        0.0         0.0           0.0            0.0   \n",
            "1989-01-10       0.0        0.0         0.0           0.0            0.0   \n",
            "\n",
            "            moving_avg_365   close  \n",
            "Date                                \n",
            "1989-01-04             0.0  2177.7  \n",
            "1989-01-05             0.0  2190.5  \n",
            "1989-01-06             0.0  2194.3  \n",
            "1989-01-09             0.0  2199.5  \n",
            "1989-01-10             0.0  2193.2  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training a simple neural network**"
      ],
      "metadata": {
        "id": "pd193UAf1HOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training a simple neural network.\n",
        "data_raw = pd.read_csv('/content/19880101-20191231.csv', index_col ='Date')\n",
        "data = generate_features(data_raw)\n",
        "\n",
        "#We construct the training set using data from 1988 to 2018 and the testing set using data from 2019:\n",
        "start_train ='1988-01-01'\n",
        "end_train   = '2018-12-31'\n",
        "start_test = '2019-01-01'\n",
        "end_test    =  '2019-12-31'\n",
        "data_train = data.loc[start_train:end_train]\n",
        "X_train = data_train.drop('close', axis=1).values\n",
        "y_train  = data_train['close'].values\n",
        "data_test = data.loc[start_test:end_test]\n",
        "X_test = data_test.drop('close', axis=1).values\n",
        "y_test  = data_test['close'].values\n",
        "\n",
        "\n",
        "#We need to normalize features into the same or a comparable scale. We do so by removing the mean and rescaling to unit variance:\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "#rescaling dataset\n",
        "X_scaled_train = scaler.fit_transform(X_train)\n",
        "X_scaled_test  = scaler.transform(X_test)\n",
        "\n",
        "#defining the model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model = Sequential([ \n",
        "        Dense(units = 32, activation = 'relu'),\n",
        "        Dense (units=1)\n",
        "        ])\n",
        "#compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer = tf.keras.optimizers.Adam(0.1))\n",
        "\n",
        "#training the model\n",
        "model.fit(X_scaled_train, y_train, epochs=100, verbose = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGPIFkFiO7T8",
        "outputId": "038425e7-d8b5-47d9-81d5-7e87fe9eb861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "237/237 [==============================] - 1s 2ms/step - loss: 34137876.0000\n",
            "Epoch 2/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 1693877.5000\n",
            "Epoch 3/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 301743.8125\n",
            "Epoch 4/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 112353.3047\n",
            "Epoch 5/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 56634.0430\n",
            "Epoch 6/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 34720.4570\n",
            "Epoch 7/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 27838.8906\n",
            "Epoch 8/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23057.2441\n",
            "Epoch 9/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22389.4688\n",
            "Epoch 10/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23434.8633\n",
            "Epoch 11/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 26015.3184\n",
            "Epoch 12/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24164.5059\n",
            "Epoch 13/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22795.5039\n",
            "Epoch 14/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22991.2715\n",
            "Epoch 15/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 27977.3008\n",
            "Epoch 16/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 26255.9258\n",
            "Epoch 17/100\n",
            "237/237 [==============================] - 1s 2ms/step - loss: 26035.1016\n",
            "Epoch 18/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 23799.2852\n",
            "Epoch 19/100\n",
            "237/237 [==============================] - 1s 2ms/step - loss: 25693.8145\n",
            "Epoch 20/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 26259.1465\n",
            "Epoch 21/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22527.3398\n",
            "Epoch 22/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 25752.4336\n",
            "Epoch 23/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 28399.8477\n",
            "Epoch 24/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24308.3457\n",
            "Epoch 25/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 26666.0234\n",
            "Epoch 26/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 25861.5234\n",
            "Epoch 27/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 25885.7324\n",
            "Epoch 28/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 28610.7949\n",
            "Epoch 29/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23579.3809\n",
            "Epoch 30/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 34704.7773\n",
            "Epoch 31/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24041.4277\n",
            "Epoch 32/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22523.5312\n",
            "Epoch 33/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 25100.1992\n",
            "Epoch 34/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23196.5723\n",
            "Epoch 35/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22968.6250\n",
            "Epoch 36/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24245.8438\n",
            "Epoch 37/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 25662.2871\n",
            "Epoch 38/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21644.7109\n",
            "Epoch 39/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23678.0234\n",
            "Epoch 40/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23398.4883\n",
            "Epoch 41/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21835.2148\n",
            "Epoch 42/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 26653.4434\n",
            "Epoch 43/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22134.1836\n",
            "Epoch 44/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23704.8926\n",
            "Epoch 45/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 22518.2344\n",
            "Epoch 46/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 25705.8379\n",
            "Epoch 47/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 21128.5234\n",
            "Epoch 48/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 21412.0859\n",
            "Epoch 49/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 22635.1523\n",
            "Epoch 50/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23182.0410\n",
            "Epoch 51/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22547.7305\n",
            "Epoch 52/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24681.5586\n",
            "Epoch 53/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20757.4258\n",
            "Epoch 54/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22665.1602\n",
            "Epoch 55/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24331.2363\n",
            "Epoch 56/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20068.0820\n",
            "Epoch 57/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19737.9004\n",
            "Epoch 58/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24388.6973\n",
            "Epoch 59/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21962.7969\n",
            "Epoch 60/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22575.9531\n",
            "Epoch 61/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19877.5059\n",
            "Epoch 62/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21066.8945\n",
            "Epoch 63/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19867.3398\n",
            "Epoch 64/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20883.7949\n",
            "Epoch 65/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20217.3613\n",
            "Epoch 66/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21218.0020\n",
            "Epoch 67/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22103.9902\n",
            "Epoch 68/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23289.4707\n",
            "Epoch 69/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 17509.3301\n",
            "Epoch 70/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22152.7539\n",
            "Epoch 71/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 28726.8086\n",
            "Epoch 72/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21406.3184\n",
            "Epoch 73/100\n",
            "237/237 [==============================] - 1s 2ms/step - loss: 19588.0391\n",
            "Epoch 74/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 20365.3027\n",
            "Epoch 75/100\n",
            "237/237 [==============================] - 1s 2ms/step - loss: 18913.4688\n",
            "Epoch 76/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 17422.4512\n",
            "Epoch 77/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 21764.8965\n",
            "Epoch 78/100\n",
            "237/237 [==============================] - 1s 3ms/step - loss: 21385.6094\n",
            "Epoch 79/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20615.9082\n",
            "Epoch 80/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20555.6484\n",
            "Epoch 81/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 22171.1484\n",
            "Epoch 82/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20131.6055\n",
            "Epoch 83/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19389.6973\n",
            "Epoch 84/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 21383.5684\n",
            "Epoch 85/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19437.4492\n",
            "Epoch 86/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19315.6016\n",
            "Epoch 87/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20324.7266\n",
            "Epoch 88/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 23518.1680\n",
            "Epoch 89/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20156.3008\n",
            "Epoch 90/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 18820.1113\n",
            "Epoch 91/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 18294.2363\n",
            "Epoch 92/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 24147.5391\n",
            "Epoch 93/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19718.8496\n",
            "Epoch 94/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20123.2520\n",
            "Epoch 95/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20623.8457\n",
            "Epoch 96/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 17929.7168\n",
            "Epoch 97/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20177.5215\n",
            "Epoch 98/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 19245.4219\n",
            "Epoch 99/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 17554.9238\n",
            "Epoch 100/100\n",
            "237/237 [==============================] - 0s 2ms/step - loss: 20004.2441\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8634f8f0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying metrics for testing the effectiveness of the model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "predictions = model.predict(X_scaled_test)\n",
        "print(f'Mse : {mean_squared_error(y_test, predictions):.3f}')\n",
        "print(f'MAE : {mean_absolute_error(y_test, predictions):.3f}')\n",
        "print(f'R^2 : {r2_score(y_test, predictions):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul7YZnolPb5F",
        "outputId": "d1d66d2f-cfe4-4c5b-f537-6e7af0df3c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "Mse : 238278.926\n",
            "MAE : 451.510\n",
            "R^2 : 0.795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine tuning the neural network**"
      ],
      "metadata": {
        "id": "Vmq5Num92wpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning the network\n",
        "from tensorboard.plugins.hparams.summary_v2 import hparams\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the hyperparameters and their values\n",
        "HP_HIDDEN = hp.HParam('hidden_size', hp.Discrete([64, 32, 16]))\n",
        "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([300, 1000]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.01, 0.4))\n",
        "\n",
        "# Define a function to train and test the model with the given hyperparameters and log the results to TensorBoard\n",
        "def train_test_model(hparams, logdir):\n",
        "    # Define the model architecture\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=hparams[HP_HIDDEN], activation='relu'),\n",
        "        tf.keras.layers.Dense(units=1)\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),\n",
        "        metrics=['mean_squared_error']\n",
        "    )\n",
        "    # Fit the model to the training data and evaluate on the test data\n",
        "    model.fit(\n",
        "        X_scaled_train, y_train,\n",
        "        validation_data=(X_scaled_test, y_test),\n",
        "        epochs=hparams[HP_EPOCHS],\n",
        "        verbose=False,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.TensorBoard(logdir),\n",
        "            hp.KerasCallback(logdir, hparams),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=200, verbose=0, mode='auto')\n",
        "        ]\n",
        "    )\n",
        "    _, mse = model.evaluate(X_scaled_test, y_test)\n",
        "    pred = model.predict(X_scaled_test)\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    return mse, r2\n",
        "\n",
        "# Define a function to run the training and testing for all combinations of hyperparameters and log the results to TensorBoard\n",
        "def run(logdir, hparams):\n",
        "    with tf.summary.create_file_writer(logdir).as_default():\n",
        "        hp.hparams(hparams)\n",
        "        mse, r2 = train_test_model(hparams, logdir)\n",
        "        tf.summary.scalar('mean_squared_error', mse, step=1)\n",
        "        tf.summary.scalar('r2', r2, step=1)\n",
        "\n",
        "# Define the range of hyperparameters to search over\n",
        "session_num = 0\n",
        "for hidden_size in HP_HIDDEN.domain.values:\n",
        "    for epochs in HP_EPOCHS.domain.values:\n",
        "        for learning_rate in tf.linspace(HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value, 5):\n",
        "            hparams = {\n",
        "                HP_HIDDEN: hidden_size,\n",
        "                HP_EPOCHS: epochs,\n",
        "                HP_LEARNING_RATE: float(\"%.2f\" % float(learning_rate))\n",
        "            }\n",
        "            run_name = \"run-%d\" % session_num\n",
        "            print('Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            run('logs/hparams_tuning/' + run_name, hparams)\n",
        "            session_num += 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcV7U-qYSFfx",
        "outputId": "1794758f-1766-4e19-d73c-486a9565a4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting trial: run-0\n",
            "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 37129.8867 - mean_squared_error: 37129.8867\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-1\n",
            "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 107588.8594 - mean_squared_error: 107588.8594\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Starting trial: run-2\n",
            "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 111543.0234 - mean_squared_error: 111543.0234\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-3\n",
            "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 101496.3906 - mean_squared_error: 101496.3906\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-4\n",
            "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 108168.6016 - mean_squared_error: 108168.6016\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-5\n",
            "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 204530.4062 - mean_squared_error: 204530.4062\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Starting trial: run-6\n",
            "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 45470.8086 - mean_squared_error: 45470.8086\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-7\n",
            "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 61280.8203 - mean_squared_error: 61280.8203\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-8\n",
            "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 141137.9844 - mean_squared_error: 141137.9844\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-9\n",
            "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 48309.7266 - mean_squared_error: 48309.7266\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-10\n",
            "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 163599.9062 - mean_squared_error: 163599.9062\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-11\n",
            "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 52317.0430 - mean_squared_error: 52317.0430\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-12\n",
            "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 81847.4609 - mean_squared_error: 81847.4609\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-13\n",
            "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 129573.2969 - mean_squared_error: 129573.2891\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-14\n",
            "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 66149.8594 - mean_squared_error: 66149.8594\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-15\n",
            "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 72871.1094 - mean_squared_error: 72871.1094\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-16\n",
            "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 58192.2031 - mean_squared_error: 58192.2031\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-17\n",
            "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 32351.0078 - mean_squared_error: 32351.0078\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-18\n",
            "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 34897.2734 - mean_squared_error: 34897.2734\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-19\n",
            "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 32656.7969 - mean_squared_error: 32656.7969\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-20\n",
            "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 142871.1562 - mean_squared_error: 142871.1562\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-21\n",
            "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 92814.8750 - mean_squared_error: 92814.8750\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-22\n",
            "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 87610.3828 - mean_squared_error: 87610.3828\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-23\n",
            "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 66222.4609 - mean_squared_error: 66222.4609\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-24\n",
            "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 62219.7695 - mean_squared_error: 62219.7695\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-25\n",
            "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.01}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 164209.0156 - mean_squared_error: 164209.0156\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Starting trial: run-26\n",
            "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.11}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 72768.5703 - mean_squared_error: 72768.5703\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-27\n",
            "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.21}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 41705.2930 - mean_squared_error: 41705.2930\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-28\n",
            "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.3}\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 59833.0312 - mean_squared_error: 59833.0312\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Starting trial: run-29\n",
            "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.4}\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 52418.5430 - mean_squared_error: 52418.5430\n",
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we found out the optimal combination of hidden_size = 16, epochs = 1000, learning rate = 0.21 is the best performing parameters which achieve the r2 score has 0.971"
      ],
      "metadata": {
        "id": "eJcQXULWuBL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we use the optimal model to make predictions:\n",
        "model=Sequential([Dense(units=16, activation='relu'), Dense(units=1)])\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.21))\n",
        "model.fit(X_scaled_train, y_train, epochs=1000, verbose=False)\n",
        "predictions_4= model.predict(X_scaled_test)[:,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAvCdeNMl0rF",
        "outputId": "0d888307-4200-4840-fd57-2fcfd2e32b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the prediction along with the ground truth as follows:\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(data_test.index, y_test, c='k')\n",
        "plt.plot(data_test.index,predictions_4,c='b')\n",
        "plt.plot(data_test.index,predictions_4,c='r')\n",
        "plt.plot(data_test.index,predictions_4,c='g')\n",
        "plt.xticks(range(0,252,10), rotation=60)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close price')\n",
        "plt.legend(['Truth', 'Neural network prediction'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "4085BejnqJnA",
        "outputId": "9b23ef94-1543-4fe0-c8fe-e52aac100e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}